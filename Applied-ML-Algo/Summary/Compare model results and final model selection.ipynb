{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Compare model results and final model selection\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "Accuracy:\n",
    "1. How do they handle data of different sizes, such as short and fat, long and skinny?\n",
    "2. How will they handle the complexity of feature relationships?\n",
    "3. How will they handle messy data?\n",
    "\n",
    "Latency:\n",
    "1. How long will it take to train?\n",
    "2. How long will it take to predict?\n",
    "\n",
    "Which algorithm generates the best model for this given problem?\n",
    "Using the tendencies and strenghts of the algorithms, i can narrow down the algorithms conidered for a given problem.\n",
    "\n",
    "Bottom line is, sometimes i will just not know which algorithm will perform best.\n",
    "\n",
    "In this section, I will do the following:\n",
    "1. Evaluate all of my saved models on the validation set\n",
    "2. Select the best model based on performance on the validation set\n",
    "3. Evaluate that model on the holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmuntaner\\Anaconda3\\envs\\ML-env\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time\n",
    "\n",
    "val_features = pd.read_csv('../data/val_features.csv')\n",
    "val_labels = pd.read_csv('../data/val_labels.csv')\n",
    "\n",
    "te_features = pd.read_csv('../data/test_features.csv')\n",
    "te_labels = pd.read_csv('../data/test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ease, since they were all saved to the same name template, i will read these-\n",
    "# in using a loop.\n",
    "\n",
    "# then i will store these model objects in a dictionary. so the dictionary will have-\n",
    "# model name as the key, and model object as the value. \n",
    "\n",
    "# in the next step, i can take those model objects and make predictions with them.\n",
    "\n",
    "# so i have this list of model names that i can loop through and then all i need-\n",
    "# to do is enter the location within joblib.load to pull in those pickled model objects-\n",
    "# and store them in the dictionary. \n",
    "\n",
    "# what the curly braces will do is it will allow me to format this string by passing in the individual model names. \n",
    "models = {}\n",
    "\n",
    "for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB']:\n",
    "    models[mdl] = joblib.load('../data/{}_model.pkl'.format(mdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR': LogisticRegression(C=1),\n",
       " 'SVM': SVC(C=0.1, kernel='linear'),\n",
       " 'MLP': MLPClassifier(activation='tanh', learning_rate='adaptive'),\n",
       " 'RF': RandomForestClassifier(max_depth=4, n_estimators=50),\n",
       " 'GB': GradientBoostingClassifier(n_estimators=50)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models on the validation set\n",
    "\n",
    "![Evaluation Metrics](../img/eval_metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that will help me evaluate each of my 5 models on the validation set. \n",
    "\n",
    "# accepts the following arguments: name of the model, the model object itself-\n",
    "# features, so this will be either the validation set or the test set,-\n",
    "# and labels, validation labes or test set labels\n",
    "\n",
    "# all the time method does is store the time whenever it is called.\n",
    "# so what i'm going to do is between the start and end, i'm going to add the predict functionality.\n",
    "\n",
    "# i'll store the time immediately before the predict call, and the time immediately after-\n",
    "# the predict call so that ican calculate how long it took to make the actual predictions\n",
    "\n",
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred), 3)\n",
    "    recall = round(recall_score(labels, pred), 3)\n",
    "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n",
    "                                                                                   accuracy,\n",
    "                                                                                   precision,\n",
    "                                                                                   recall,\n",
    "                                                                                   round((end - start)*1000, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "Now i have the models dictionary, where the keys are the model names and the values are the stored objects.\n",
    "\n",
    "Now i will loop through this models dictionary, i will want to extract the name and the model, so thats the key and the value of the dictionary.\n",
    "I do this by calling models.items.\n",
    "\n",
    "And its that items call that helps to split out the key and the value. Now that i have name and model object i have my features and labels i can call the evaluate function.\n",
    "\n",
    "Within each loop, i will call evaluate model, pass in the name first, and then the model object and right now i want to evaluate this on the validation set, so i'll pass in validation features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR -- Accuracy: 0.775 / Precision: 0.712 / Recall: 0.646 / Latency: 2.0ms\n",
      "SVM -- Accuracy: 0.747 / Precision: 0.672 / Recall: 0.6 / Latency: 3.0ms\n",
      "MLP -- Accuracy: 0.781 / Precision: 0.717 / Recall: 0.662 / Latency: 8.0ms\n",
      "RF -- Accuracy: 0.809 / Precision: 0.792 / Recall: 0.646 / Latency: 6.0ms\n",
      "GB -- Accuracy: 0.809 / Precision: 0.804 / Recall: 0.631 / Latency: 1.0ms\n"
     ]
    }
   ],
   "source": [
    "for name, mdl in models.items():\n",
    "    evaluate_model(name, mdl, val_features, val_labels)\n",
    "    \n",
    "# before i continue into the results, theres one important thing to note.\n",
    "# i have mentioned before that if i ran random forest twice, i would get-\n",
    "# different results. \n",
    "\n",
    "# its critical to understand that this was only in the training phase.\n",
    "# what i am dealing with now is a stored fit concrete model.\n",
    "\n",
    "# at this point if i run this cell twice, i'll get the same results. \n",
    "# the only difference might be the latency but the actual accuracy will remain the same.\n",
    "\n",
    "# Couple things to note here:\n",
    "# 1st, the Gradient booster model is generating the best results on this unseen data.\n",
    "# it has the best accuracy, precision, but slightly lower recall than  random forest.\n",
    "\n",
    "# 2nd, Random forest takes the longest to make predictions, so this brings me to a conversation about trade-offs\n",
    "# there are 2 types of trade-offs. The first is precision vs recall.\n",
    "\n",
    "# typically i will have to make a choice between what i rather have in my model. this will all-\n",
    "# depend on the question i want to answer or the business case. \n",
    "\n",
    "# for instance if this is a spam detection problem, then i would optimize for precision.\n",
    "# in other words, if my model says that its spam, it better be spam or else i'll be-\n",
    "# blocking real emails that people would like to see. \n",
    "\n",
    "# on the other side, if this is a fraud detection model, i would more likely optimize recall,-\n",
    "# because missing any of these real fraudulent transactions could cost thousands of dollars\n",
    "\n",
    "# The second trade-off is between overall accuracy, and when i say overall accuracy, I mean precision recall and accuracy-\n",
    "# and latency.\n",
    "\n",
    "# In my case, my best model based on accuracy is also the slowest model to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost -- Accuracy: 0.816 / Precision: 0.852 / Recall: 0.684 / Latency: 2.0ms\n"
     ]
    }
   ],
   "source": [
    "# I should see performance that aligns fairly closely with the validation set.\n",
    "\n",
    "# the reason i evaluate both on the validation set and the test set is -\n",
    "# because i used performance on the validation set to select my best model.\n",
    "\n",
    "# in a sense, the validation set played a role in my selection of what my best model was-\n",
    "# for this problem. \n",
    "\n",
    "# this test set will not be used for any kind of model selection. so it is a completely-\n",
    "# unbiased view of how i can expect this model to perform moving forward.\n",
    "evaluate_model('Gradient Boost', models['GB'], te_features, te_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can see that performance is relatively close to what i saw on the validation set\n",
    "# Accuracy is slightly higher along with precision and recall\n",
    "\n",
    "# So now i have a great feel for the likely performance of the model on new data.\n",
    "# I can be confident in proposing this model as the best model for making predictions on whether people-\n",
    "# aboard the titanic will survive or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
