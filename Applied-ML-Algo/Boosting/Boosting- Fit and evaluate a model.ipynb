{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: Fit and evaluate a model\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will fit and evaluate a simple Gradient Boosting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "tr_features = pd.read_csv('../data/train_features.csv')\n",
    "tr_labels = pd.read_csv('../data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning:\n",
    "n_estimators simply represents how many individual decision trees i need to build.\n",
    "\n",
    "max_depth indicates how deep each of those trees will go.\n",
    "\n",
    "learning_rate controls how quickly this algorithm will try to find the optimal solution.\n",
    "If i set it too large it may never find it, too small and it also may never find the optimal solution. \n",
    "![GB](../img/gb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "\n",
      "0.624 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.796 (+/-0.115) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.796 (+/-0.115) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.811 (+/-0.117) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.624 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.811 (+/-0.069) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.83 (+/-0.074) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.839 (+/-0.084) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.624 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.822 (+/-0.052) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.82 (+/-0.037) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.826 (+/-0.047) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.624 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.817 (+/-0.049) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.822 (+/-0.039) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.8 (+/-0.035) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.624 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.8 (+/-0.052) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.801 (+/-0.042) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.792 (+/-0.044) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.796 (+/-0.115) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.815 (+/-0.119) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.818 (+/-0.111) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.828 (+/-0.092) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.813 (+/-0.071) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.841 (+/-0.07) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.83 (+/-0.035) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.811 (+/-0.036) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.813 (+/-0.045) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.82 (+/-0.037) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.807 (+/-0.03) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.796 (+/-0.039) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.818 (+/-0.045) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.803 (+/-0.021) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.8 (+/-0.034) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.794 (+/-0.037) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.798 (+/-0.048) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.781 (+/-0.023) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.801 (+/-0.052) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.787 (+/-0.03) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.818 (+/-0.099) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.832 (+/-0.081) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.826 (+/-0.077) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.822 (+/-0.081) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.82 (+/-0.061) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.792 (+/-0.042) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.792 (+/-0.033) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.794 (+/-0.024) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.794 (+/-0.034) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.787 (+/-0.018) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.794 (+/-0.045) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.794 (+/-0.027) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.798 (+/-0.044) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.801 (+/-0.057) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.798 (+/-0.039) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.801 (+/-0.037) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.777 (+/-0.05) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.801 (+/-0.056) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.775 (+/-0.059) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.8 (+/-0.044) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.204 (+/-0.115) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.204 (+/-0.115) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.204 (+/-0.115) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.204 (+/-0.115) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.307 (+/-0.195) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.307 (+/-0.195) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.307 (+/-0.195) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.307 (+/-0.195) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.442 (+/-0.247) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.378 (+/-0.201) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.369 (+/-0.169) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.367 (+/-0.163) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.592 (+/-0.177) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.597 (+/-0.167) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.639 (+/-0.105) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.618 (+/-0.185) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.706 (+/-0.125) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.687 (+/-0.118) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.699 (+/-0.146) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.712 (+/-0.116) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.376 (+/-0.007) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.376 (+/-0.007) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.376 (+/-0.007) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.376 (+/-0.007) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.29 (+/-0.102) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.29 (+/-0.102) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.29 (+/-0.102) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.29 (+/-0.102) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.354 (+/-0.182) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.352 (+/-0.193) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.348 (+/-0.176) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.348 (+/-0.176) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.556 (+/-0.113) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.588 (+/-0.096) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.569 (+/-0.116) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.579 (+/-0.104) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.689 (+/-0.119) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.655 (+/-0.069) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.667 (+/-0.095) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.687 (+/-0.08) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# As i mentioned before, I am testing out more individual decision trees but shallower-\n",
    "# trees than i did with random forest.\n",
    "\n",
    "# each of these two algorithms optimize the bias varience trade-off to find the best model.\n",
    "\n",
    "# gradient boodting does better with a lot of shallow trees, while random forest goes deeper on fewer trees. \n",
    "gb = GradientBoostingClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 250, 500],\n",
    "    'max_depth': [1, 3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(gb, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "So the same caveat that i called out with random forest is also true here. \n",
    "\n",
    "There is a random component within each boosting model because i am sampling \n",
    "\n",
    "There are a lot of hyperparameter combinations here. Remember I tested 4 levels of n_estimators, 5 levels of max_depth, and 5 levels of learning_rate. That's 100 total models that were built. \n",
    "\n",
    "I can see that the best model has a learning rate of 0.01 with a max_depth of 3 and 50 total estimators. \n",
    "\n",
    "This model is using drastically more trees than i saw with the random forest but eac individual tree is more shallow than what random forest settled on as its best model. \n",
    "\n",
    "This optimal hyperparameter combination is generating an overall accuracy of 84.1%. This is better than any Of the other algorithms that i have tested. \n",
    "\n",
    "I want to highlight the fact that this does not mean that this is the best model.It just means its the best using cross-validation.\n",
    "\n",
    "Evaluating it on the test set like i'll do after this, will be the true test. \n",
    "\n",
    "Two things to take ito account for these results is that high learning rate is generating really poor results across the board. That indicates that its jumping across the loss curve too quickly and not finding the optimal model.\n",
    "\n",
    "Another thing is that the models that are only using 5 decision trees is pretty consistantly generating the worst results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out pickled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/GB_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cv.best_estimator_, '../data/GB_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
