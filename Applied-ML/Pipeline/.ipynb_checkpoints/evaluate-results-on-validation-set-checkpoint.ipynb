{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Evaluate results on validation set\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, I will use what I learned in last section to fit the best few models on the full training set and then evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmuntaner\\Anaconda3\\envs\\ML-env\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "tr_features = pd.read_csv('../data/train_features.csv')\n",
    "tr_labels = pd.read_csv('../data/train_labels.csv')\n",
    "\n",
    "val_features = pd.read_csv('../data/val_features.csv')\n",
    "val_labels = pd.read_csv('../data/val_labels.csv')\n",
    "\n",
    "te_features = pd.read_csv('../data/test_features.csv')\n",
    "te_labels = pd.read_csv('../data/test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit best models on full training set\n",
    "\n",
    "Results from last section:\n",
    "```\n",
    "BEST PARAMS: {'max_depth': 10, 'n_estimators': 100}\n",
    "\n",
    "0.772 (+/-0.068) for {'max_depth': 2, 'n_estimators': 5}\n",
    "0.805 (+/-0.098) for {'max_depth': 2, 'n_estimators': 50}\n",
    "0.792 (+/-0.144) for {'max_depth': 2, 'n_estimators': 100}\n",
    "0.794 (+/-0.052) for {'max_depth': 10, 'n_estimators': 5}\n",
    "0.82 (+/-0.057) for {'max_depth': 10, 'n_estimators': 50}\n",
    "---0.832 (+/-0.054) for {'max_depth': 10, 'n_estimators': 100}\n",
    "0.801 (+/-0.052) for {'max_depth': 20, 'n_estimators': 5}\n",
    "0.807 (+/-0.034) for {'max_depth': 20, 'n_estimators': 50}\n",
    "0.803 (+/-0.026) for {'max_depth': 20, 'n_estimators': 100}\n",
    "0.794 (+/-0.06) for {'max_depth': None, 'n_estimators': 5}\n",
    "---0.807 (+/-0.014) for {'max_depth': None, 'n_estimators': 50}\n",
    "---0.809 (+/-0.033) for {'max_depth': None, 'n_estimators': 100}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i will take the 3 best hyper parameter combinations and i will re fit those models on the-\n",
    "# full training set.\n",
    "\n",
    "# why do i need to refit on the full training set?\n",
    "# i want to evaluate this on the validation set, so i will allow my model to learn from-\n",
    "# the full training set, instead of limiting it to only 80%\n",
    "\n",
    "# once this cell is run, rf1, rf2, rf3 will become fit models. i can then use them to make-\n",
    "# predictions on unseen data\n",
    "rf1 = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rf1.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators=100, max_depth=None)\n",
    "rf2.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "rf3 = RandomForestClassifier(n_estimators=50, max_depth=None)\n",
    "rf3.fit(tr_features, tr_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models on validation set\n",
    "Now i will evealuate the above fit models on the validation set.\n",
    "The only examples these models have seen at this point are those in the training set.\n",
    "\n",
    "This is the true test. its the test of the models ability to generalize to unseen data. \n",
    "If they are overfit or underfit, they will fail here.\n",
    "\n",
    "I will be using accuracy, precision, and recall to evaluate these models to select the one that generalizes best to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX DEPTH: 10 / # OF EST: 100 -- A: 0.821 / P: 0.824 / R: 0.737\n",
      "MAX DEPTH: None / # OF EST: 100 -- A: 0.799 / P: 0.786 / R: 0.724\n",
      "MAX DEPTH: None / # OF EST: 50 -- A: 0.821 / P: 0.824 / R: 0.737\n"
     ]
    }
   ],
   "source": [
    "# to eveluate these i will use a for loop where i will cycle through my three models.\n",
    "# and then for each model, i will call the predict method, and then pass in my validation features.\n",
    "# this will make predictions and it will output an array of those features \n",
    "\n",
    "# after i have the predictions, i want to generate some results metrics\n",
    "for mdl in [rf1, rf2, rf3]:\n",
    "    y_pred = mdl.predict(val_features)\n",
    "    accuracy = round(accuracy_score(val_labels, y_pred), 3)\n",
    "    precision = round(precision_score(val_labels, y_pred), 3)\n",
    "    recall = round(recall_score(val_labels, y_pred), 3)\n",
    "    print('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(mdl.max_depth,\n",
    "                                                                         mdl.n_estimators,\n",
    "                                                                         accuracy,\n",
    "                                                                         precision,\n",
    "                                                                         recall))\n",
    "\n",
    "# i can see that the model that performed best in cross validation actually did not-\n",
    "# perform best in the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best model on the test set\n",
    "Up to this point i have dont the following:\n",
    "\n",
    "1. Explore & clean the data\n",
    "2. Split data into train/validation/test\n",
    "3. Fit an initial model and evaluate\n",
    "4. Tune hyper parameters \n",
    "5. Evaluate on Validation Set\n",
    "6. Final model selection and evaluation on test set\n",
    "\n",
    "Now that i have the best model, i need to evaluate it on a test set to get a truly unbiased view of how it should perform moving forward. \n",
    "\n",
    "So this final test set is purely for evaluation purposes to see  that it matches the performance that i have seen before. And to give me more confidence in the performance of the model moving forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX DEPTH: 10 / # OF EST: 100 -- A: 0.792 / P: 0.741 / R: 0.662\n"
     ]
    }
   ],
   "source": [
    "# i am using my best model which was rf1, and that was already fit on the full training set.\n",
    "# i will make predictions using my test features\n",
    "y_pred = rf1.predict(te_features)\n",
    "accuracy = round(accuracy_score(te_labels, y_pred), 3)\n",
    "precision = round(precision_score(te_labels, y_pred), 3)\n",
    "recall = round(recall_score(te_labels, y_pred), 3)\n",
    "print('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(rf1.max_depth,\n",
    "                                                                     rf1.n_estimators,\n",
    "                                                                     accuracy,\n",
    "                                                                     precision,\n",
    "                                                                     recall))\n",
    "\n",
    "# looking at these performance metrics and circling all the way back to grid search cv-\n",
    "# the model with these hyper parameter settings had 83.2% accuracy for grid search cv, 82.1% on validation set,\n",
    "# and 79.2% accuracy on this test set.\n",
    "\n",
    "# this highlights how the performance of a given model can vary based on the data i give it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
