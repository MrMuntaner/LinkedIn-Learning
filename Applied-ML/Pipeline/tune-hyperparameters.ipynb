{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Tune hyperparameters\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, I will tune the hyperparameters for the basic model we fit in the last section.\n",
    "\n",
    "I will add one more layer to this by running grid search to find the optimal hyper-parameter settings for my mode.\n",
    "\n",
    "I wan tto beat the baseline perfomance from the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tr_features = pd.read_csv('../data/train_features.csv')\n",
    "tr_labels = pd.read_csv('../data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "The gridsearchcv method stores a lot of information about model performance. But it can be difficult to pick through that to find what i really need.\n",
    "\n",
    "The function below helps print out the results a bit more cleanly. In essence what it does is for every hyper parameter combination, it will print out the average accuracy score across the 5 folds, and the standard dev of the accuracy score across those 5 folds.\n",
    "\n",
    "This will give me all the info i need to select the optimal hyper parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'max_depth': 10, 'n_estimators': 100}\n",
      "\n",
      "0.772 (+/-0.068) for {'max_depth': 2, 'n_estimators': 5}\n",
      "0.805 (+/-0.098) for {'max_depth': 2, 'n_estimators': 50}\n",
      "0.792 (+/-0.144) for {'max_depth': 2, 'n_estimators': 100}\n",
      "0.794 (+/-0.052) for {'max_depth': 10, 'n_estimators': 5}\n",
      "0.82 (+/-0.057) for {'max_depth': 10, 'n_estimators': 50}\n",
      "0.832 (+/-0.054) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.801 (+/-0.052) for {'max_depth': 20, 'n_estimators': 5}\n",
      "0.807 (+/-0.034) for {'max_depth': 20, 'n_estimators': 50}\n",
      "0.803 (+/-0.026) for {'max_depth': 20, 'n_estimators': 100}\n",
      "0.794 (+/-0.06) for {'max_depth': None, 'n_estimators': 5}\n",
      "0.807 (+/-0.014) for {'max_depth': None, 'n_estimators': 50}\n",
      "0.809 (+/-0.033) for {'max_depth': None, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# now i can do the actual grid search.\n",
    "# essentialy random forest is a collection of decision trees.\n",
    "\n",
    "# there are two hyper parameter settings that i want to tune.\n",
    "# number of estimators so this means how many individual decision trees do i want to build within my-\n",
    "# random forest.\n",
    "\n",
    "# secondly is max depth, which will dictate how deep each individual decision trees go. \n",
    "# (None) will allow it to just grow the decision tree as deep as it needs to go until it hits-\n",
    "# some stopping criteria thats built in within the decision tree.\n",
    "rf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 100],\n",
    "    'max_depth': [2, 10, 20, None]\n",
    "}\n",
    "\n",
    "# pass in the model object, parameter dictionary, number of folds i want \n",
    "cv = GridSearchCV(rf, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "# what this will do under the hood, is it will take each parameter combination, so theres 3-\n",
    "# levels of estimator, and 4 levels of max depth, so thats 12 total combinations.\n",
    "# for each combination, its running 5 fold cross validation.\n",
    "\n",
    "# essentially this is building 60 models under the hood. \n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
